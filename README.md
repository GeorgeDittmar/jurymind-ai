![alt text](https://github.com/GeorgeDittmar/jurymind-ai/blob/main/logo.png?raw=true)

**Collective AI Judgment for Smarter Models**

---

## Overview

JuryMind AI is an evaluation-as-a-service platform designed to harness the power of large language models (LLMs) as intelligent judges. Our platform enables automated LLM evaluation, prompt optimization, and auto-labeling with agentic AI judges working collaboratively â€” like a jury of experts.

JuryMind AI empowers ML teams, AI researchers, and startups to measure, refine, and improve their language models and prompt engineering workflows with minimal manual effort.

---

## Features

- **LLM Evaluation:** Score model outputs based on customizable criteria using expert LLM judges.
- **Prompt Optimization:** Iteratively improve prompts to achieve specified goals like clarity, relevance, or conciseness.
- **Dataset Generation** Generate high-quality datasets over your data.
- **Auto Labeling:** Generate high-quality labels for datasets automatically using AI-driven judgments.
- **Agentic Judges:** Leverage multiple AI agents working in parallel or consensus for robust evaluations.
- **Modular Architecture:** Easily extend the platform with modules like JudgeLab(TBD), PromptLab(TBD), and LabelLab.

---

## Getting Started

### Prerequisites

- Python 3.8+
- Node.js 16+
- Docker & Docker Compose
- OpenAI API key (set in `.env`)

### Installation

1. Clone the repo:
   ```bash
   git clone https://github.com/yourusername/jurymind-ai.git
   cd jurymind-ai
